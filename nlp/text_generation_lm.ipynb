{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d71c9e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, GRU, Embedding\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import to_categorical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c6bdc3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data\n",
    "f = open('../../Data/text-data/rnn-text-file.txt', 'r', encoding='utf-8')\n",
    "data_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "98529a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    We perform basic text preprocessing since this data does not have much noise. We lower case all the words to maintain \n",
    "    uniformity.\n",
    "'''\n",
    "\n",
    "def text_cleaner(text):\n",
    "    # lower case text\n",
    "    newString = text.lower().strip()    \n",
    "    # remove punctuations\n",
    "    newString = re.sub(\"[^a-zA-Z]\", \" \", newString)  \n",
    "    newString = re.sub(r\"'s\\b\",\" \",newString)\n",
    "    newString = re.sub(r\" +\",\" \",newString)\n",
    "    return newString\n",
    "   \n",
    "# preprocess the text\n",
    "data_new = text_cleaner(data_text)\n",
    "# print(data_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7cb79feb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sequences: 5877\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    we take in 30 characters as context and ask the model to predict the next character. Now, 30 is a number which I got \n",
    "    by trial and error and we can use any other length of context.\n",
    "\n",
    "'''\n",
    "\n",
    "def create_seq(text):\n",
    "    length = 30\n",
    "    sequences = list()\n",
    "    words = text.split(' ')\n",
    "    for i in range(length, len(words)):\n",
    "        # select sequence of tokens\n",
    "        seq = words[i-length:i+1]\n",
    "        # store\n",
    "        sequences.append(seq)\n",
    "    print('Total Sequences: %d' % len(sequences))\n",
    "    return sequences\n",
    "\n",
    "# create sequences   \n",
    "sequences = create_seq(data_new)\n",
    "# print(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1a349c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Once the sequences are generated, the next step is to encode each character. After encoding, each word here will be \n",
    "    represented as a unique number and will be ready to use form for Keras library. \n",
    "'''\n",
    "\n",
    "# create a character mapping index\n",
    "words = sorted(list(set(data_new.split(' '))))\n",
    "mapping = dict((c, i) for i, c in enumerate(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "42db814b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_seq(seq):\n",
    "    sequences = list()\n",
    "    for line in seq:\n",
    "        # integer encode line\n",
    "        encoded_seq = [mapping[char] for char in line]\n",
    "        # store\n",
    "        sequences.append(encoded_seq)\n",
    "    return sequences\n",
    "\n",
    "# encode the sequences\n",
    "sequences = encode_seq(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "673d82e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (5289, 30) Val shape: (588, 30)\n"
     ]
    }
   ],
   "source": [
    "# vocabulary size\n",
    "vocab = len(mapping)\n",
    "sequences = np.array(sequences)\n",
    "\n",
    "# create X and y\n",
    "X, y = sequences[:,:-1], sequences[:,-1]\n",
    "\n",
    "# one hot encode y\n",
    "y = to_categorical(y, num_classes=vocab)\n",
    "# create train and validation sets\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "print('Train shape:', X_tr.shape, 'Val shape:', X_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7994ef21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 30, 50)            83950     \n",
      "                                                                 \n",
      " gru (GRU)                   (None, 150)               90900     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1679)              253529    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 428379 (1.63 MB)\n",
      "Trainable params: 428379 (1.63 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/5\n",
      "166/166 - 9s - loss: 6.7457 - acc: 0.0664 - val_loss: 6.5971 - val_acc: 0.0697 - 9s/epoch - 54ms/step\n",
      "Epoch 2/5\n",
      "166/166 - 6s - loss: 6.1760 - acc: 0.0679 - val_loss: 6.5478 - val_acc: 0.0748 - 6s/epoch - 34ms/step\n",
      "Epoch 3/5\n",
      "166/166 - 6s - loss: 5.9144 - acc: 0.0862 - val_loss: 6.5469 - val_acc: 0.0799 - 6s/epoch - 35ms/step\n",
      "Epoch 4/5\n",
      "166/166 - 6s - loss: 5.6140 - acc: 0.1010 - val_loss: 6.5027 - val_acc: 0.0884 - 6s/epoch - 35ms/step\n",
      "Epoch 5/5\n",
      "166/166 - 6s - loss: 5.3033 - acc: 0.1227 - val_loss: 6.6050 - val_acc: 0.0986 - 6s/epoch - 35ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ArnabBiswas\\anaconda3\\envs\\mlenv\\Lib\\site-packages\\keras\\src\\engine\\training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "def train_model(model_name, epochs, X_train, y_train, X_val, y_val):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab, 50, input_length=30, trainable=True))\n",
    "    model.add(GRU(150, recurrent_dropout=0.1, dropout=0.1))\n",
    "    model.add(Dense(vocab, activation='softmax'))\n",
    "    print(model.summary())\n",
    "\n",
    "    # compile the model\n",
    "    model.compile(loss='categorical_crossentropy', metrics=['acc'], optimizer='adam')\n",
    "    # fit the model\n",
    "    model.fit(X_train, y_train, epochs=epochs, verbose=2, validation_data=(X_val, y_val))\n",
    "    model.save(model_name)\n",
    "    print(model.summary())\n",
    "\n",
    "train_model('model1.h5', 5, X_tr, y_tr, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "57b2095d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "# generate a sequence of characters with a language model\n",
    "def generate_seq(model, mapping, seq_length, seed_text, n_words):\n",
    "    in_text = text_cleaner(seed_text)\n",
    "    # generate a fixed number of characters\n",
    "    for _ in range(n_words):\n",
    "        # encode the characters as integers\n",
    "        encoded = [mapping[word] for word in in_text.split(' ')]\n",
    "        # truncate sequences to a fixed length\n",
    "        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
    "                \n",
    "        predict_x=model.predict(encoded) \n",
    "        yhat=np.argmax(predict_x,axis=1)\n",
    "      \n",
    "        # reverse map integer to character\n",
    "        out_char = ''\n",
    "        for char, index in mapping.items():\n",
    "            if index == yhat:\n",
    "                out_char = char\n",
    "                break\n",
    "        # append to input\n",
    "        in_text = in_text +' '+char\n",
    "\n",
    "    return in_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b66cee00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 282ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "however after minutes he made the first controversial decision pritam kotal had the ball at his feet in the box roy krishna reached the ball before the mohun bagan captain bowled it the bengaluru footballer got the ball pritam tackled his legs from behind krishna fell into the box bengaluru footballers continued to appeal for penalties but the referee did not give a penalty of the prime minister had been a large breakout of\n"
     ]
    }
   ],
   "source": [
    "model = load_model('model1.h5')\n",
    "seed_text = 'However, after 35 minutes, he made the first controversial decision. Pritam Kotal had the ball at his feet in the box.Roy Krishna reached the ball before the Mohun Bagan captain bowled it. The Bengaluru footballer got the ball. Pritam tackled his legs from behind. Krishna fell into the box. Bengaluru footballers continued to appeal for penalties. But the referee did not give a penalty'\n",
    "txt =  generate_seq(model, mapping, 30, seed_text, 10)\n",
    "print(txt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
